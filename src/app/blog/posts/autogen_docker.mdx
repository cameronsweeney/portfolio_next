---
title: 'Running Open-Source LLMs with AutoGen, Oobabooga, and Docker'
publishedAt: '2024-06-26'
summary: 'In a very technical sense, I did it.'
status: 'Published'
---

In the realm of text generation and AI-powered tools, setting up an efficient and flexible environment is key to leveraging the best of open-source models. My recent journey involved integrating AutoGen with an open-source model through a Docker container and connecting it to an Oobabooga backend. While it's possible for me to run some smaller quantized models locally on my machine (thanks, [LM Studio](https://lmstudio.ai/)!), it's nice to have a free option to run larger models in the cloud that tend to generate better results.

Here I used [Mistral 7B Instruct v0.2](https://huggingface.co/mistralai/Mistral-7B-Instruct-v0.2), but you can load just about any open-source model from Hugging Face into Oobabooga. For detailed instructions on how to replicate this setup, check out my repo here: [https://github.com/cameronsweeney/autogen_docker_test](https://github.com/cameronsweeney/autogen_docker_test).

This exploration not only deepened my understanding of these tools but also illuminated a path for others to follow with ease.

## Establishing the Oobabooga backend

The adventure starts by bringing Oobabooga to life [within a Google Colab notebook](https://colab.research.google.com/github/oobabooga/text-generation-webui/blob/main/Colab-TextGen-GPU.ipynb#scrollTo=LGQ8BiMuXMDG). This quirky-named backend acts as a crucial bridge to our desired AI model, allowing seamless interactions. This setup works great for small-scale research and testing without a Google Colab subscription, but most likely wouldn't be suitable for a production environment.

The real utility unfolds when you specify the model you wish to use and expose it through an OpenAI-like API. The model, once loaded, provides a Cloudflare URL which becomes the endpoint for our AutoGen setup, creating an interconnected high-tech system. Routing it all through an API with similar endpoints to OpenAI's API lets us test open-source models against various ChatGPT models without rewriting too much code!

## Using Docker for a smoother and more secure workflow

Building a Docker image for AutoGen introduces a layer of abstraction, encapsulating the environment and its dependencies in a container. If you want to get AutoGen to execute any of the code that an LLM generates, this is an important security feature! Containerizing with Docker also helps make code more portable across different operating systems and machines. 

This image, derived from AutoGen's development Dockerfile, encompasses everything needed to run AutoGen seamlessly. The build process, though lengthy due to the image size, is a one-time investment into creating a robust development environment. 

After the initial build (which does involve downloading several gigabytes of data), a simple `docker compose` command not only builds our custom setup but also allows for interactive engagement through the terminal. The flexibility of Docker Compose means that any modifications, either to scripts or the environment, are easily accommodated without the need to rebuild the entire image from scratch.

## AutoGen magic without the price tag

AutoGen is a pretty incredible framework for getting multiple AI agents to work together. Their repository is [full of example notebooks](https://github.com/microsoft/autogen/tree/main/notebook) that demonstrate its capabilities. Imagine a swarm of specialized agents working together towards a goal.

One downside of AutoGen is how expensive it can be with an enterprise model like ChatGPT. Because an AutoGen conversation consists of multiple agents talking to each other, it burns through tokens *very* quickly. AutoGen allows you to implement powerful patterns where LLMs critique and improve their own work automatically, but it takes a lot of back-and-forth for that to happen (much like between humans when they work collaboratively), and all of those requests to OpenAI or another paid LLM can add up quickly. Even if it's not as "smart" of a model, having a cost-free option can open a ton of doors into new possibilities!

In sharing this journey, my hope is to inspire others to embark on their own adventures in the world of AI, armed with the knowledge that with the right tools and a bit of guidance, the possibilities are endless. If you could manage a swarm of helpful AI agents, what would you have them do?